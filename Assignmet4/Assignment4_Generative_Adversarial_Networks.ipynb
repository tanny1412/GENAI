{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Theory Questions and Answers**\n",
        "\n",
        "## **Q1: Explain the minimax loss function in GANs and how it ensures competitive training between the generator and discriminator.**\n",
        "\n",
        "In **Generative Adversarial Networks (GANs)**, the **minimax loss function** defines the adversarial relationship between the **generator (G)** and the **discriminator (D)**. The objective of training a GAN is formulated as:\n",
        "\n",
        "$$\n",
        "\\min_G \\max_D V(G, D) = \\mathbb{E}_{x \\sim p_{\\text{data}}} [\\log D(x)] + \\mathbb{E}_{z \\sim p_z} [\\log (1 - D(G(z)))]\n",
        "$$\n",
        "\n",
        "This equation represents a **two-player zero-sum game** where:\n",
        "\n",
        "- The **discriminator (D)** tries to **maximize** its accuracy in distinguishing real images from fake images.\n",
        "- The **generator (G)** tries to **minimize** the loss by **fooling the discriminator** into classifying its generated images as real.\n",
        "\n",
        "### **How it Ensures Competitive Training:**\n",
        "1. **Generator Improvement** – If the discriminator is **too good**, the generator receives strong gradients and learns to produce better images.\n",
        "2. **Discriminator Improvement** – If the generator improves, the discriminator must become more **sensitive** to subtle differences between real and fake data.\n",
        "3. **Equilibrium** – Ideally, both networks improve until the generated data distribution becomes **indistinguishable** from the real distribution.\n",
        "\n",
        "However, in practice, **training instability** is common, requiring modifications like **WGAN, Feature Matching, or Spectral Normalization**.\n",
        "\n",
        "---\n",
        "\n",
        "## **Q2: What is mode collapse, why does it occur, and how can it be mitigated?**\n",
        "\n",
        "### **Mode Collapse:**\n",
        "Mode collapse occurs when the **generator fails to produce diverse outputs** and instead generates only a **few repeated patterns** to fool the discriminator. This reduces the **variety of generated images**, even if they look realistic.\n",
        "\n",
        "### **Why Does Mode Collapse Occur?**\n",
        "- **Generator Exploits Weakness in the Discriminator** – If the generator finds a small set of images that consistently fool the discriminator, it may ignore other possible variations.\n",
        "- **Poor Gradient Flow** – If the loss function leads to **vanishing gradients**, the generator might struggle to learn a broad range of features.\n",
        "- **Overfitting to a Few Features** – The generator focuses on a **small subset of the data distribution**, neglecting others.\n",
        "\n",
        "### **How to Mitigate Mode Collapse:**\n",
        " **Use Mini-batch Discrimination** – Encourages diversity by comparing images across batches.  \n",
        " **Wasserstein GAN (WGAN-GP)** – Provides smoother gradients, reducing collapse.  \n",
        " **Feature Matching Loss** – Instead of only trying to fool the discriminator, the generator is trained to **match feature statistics** of real data.  \n",
        " **Entropy Regularization** – Encourages more diverse outputs by penalizing low variation.  \n",
        "\n",
        "---\n",
        "\n",
        "## **Q3: Explain the role of the discriminator in adversarial training.**\n",
        "\n",
        "The **discriminator (D)** is a crucial part of the adversarial process in GANs. It acts as a **binary classifier** that distinguishes between **real** images (from the dataset) and **fake** images (generated by G).\n",
        "\n",
        "### **Key Functions of the Discriminator:**\n",
        "1. **Guiding the Generator** – The discriminator provides **feedback** (gradients) to the generator, helping it learn how to produce more realistic images.\n",
        "2. **Improving Itself** – It continuously **learns to better distinguish** real vs. fake images.\n",
        "3. **Adversarial Competition** – It competes with the generator, forcing it to improve.\n",
        "4. **Loss Calculation** – It uses **binary cross-entropy loss** or **Wasserstein loss** to measure how well it classifies images.\n",
        "\n",
        "### **Discriminator Training Process:**\n",
        "1. The discriminator is **trained on real images** (label = 1).\n",
        "2. It is also **trained on fake images** generated by **G** (label = 0).\n",
        "3. It updates its weights to improve classification accuracy.\n",
        "4. The **generator uses this feedback** to improve itself.\n",
        "\n",
        "If the discriminator **becomes too strong**, the generator may struggle to improve. Techniques like **label smoothing** and **gradient penalties** can help balance training.\n",
        "\n",
        "---\n",
        "\n",
        "## **Q4: How do metrics like IS and FID evaluate GAN performance?**\n",
        "\n",
        "Since GANs generate **new data**, traditional metrics like accuracy **don’t work**. Instead, **Inception Score (IS)** and **Fréchet Inception Distance (FID)** are used to evaluate **image quality and diversity**.\n",
        "\n",
        "### **1. Inception Score (IS)**\n",
        "- Measures **how realistic and diverse** the generated images are.\n",
        "- Uses a **pre-trained classifier (InceptionV3)** to predict labels for generated images.\n",
        "- A **higher IS** means the images are:\n",
        "   **High quality** (classifier is confident in predictions).  \n",
        "   **Diverse** (spread across multiple categories).  \n",
        "\n",
        "$$\n",
        "IS = \\exp \\left( \\mathbb{E}_x \\left[ KL(p(y|x) || p(y)) \\right] \\right)\n",
        "$$\n",
        "\n",
        "**Limitations:**  \n",
        "- Doesn’t compare against real images.\n",
        "- Can be **misleading** if the generator produces many distinct but unrealistic images.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Fréchet Inception Distance (FID)**\n",
        "- Measures **how close the generated images are to real images** in feature space.\n",
        "- Computes the **distance** between feature distributions of real vs. fake images using InceptionV3.\n",
        "- A **lower FID score** means:\n",
        "   The generator produces images closer to real data.  \n",
        "   The generator captures better texture and structure.  \n",
        "\n",
        "$$\n",
        "FID = || \\mu_r - \\mu_g ||^2 + \\text{Tr}(\\Sigma_r + \\Sigma_g - 2(\\Sigma_r \\Sigma_g)^{1/2})\n",
        "$$\n",
        "\n",
        "**Limitations:**  \n",
        "- Can be biased depending on dataset size.\n",
        "- Requires a good feature extractor (InceptionV3).\n"
      ],
      "metadata": {
        "id": "TeupPYA9ZGer"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mn57MLe1biPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "nud_hhkx-UHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Hyperparameters\n"
      ],
      "metadata": {
        "id": "pez51Ene-ch2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsBWKVnryAUy"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 256\n",
        "NOISE_DIM = 128   # The dimensionality of the random noise vector\n",
        "EPOCHS = 300\n",
        "BUFFER_SIZE = 50000  # CIFAR-10 has 50K training images\n",
        "SAVE_EVERY = 10      # Save generated images every 10 epochs\n",
        "NUM_EXAMPLES_TO_GENERATE = 16  # Number of images to generate for snapshot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Load and Preprocess CIFAR-10\n",
        "##    CIFAR-10 images are 32x32x3."
      ],
      "metadata": {
        "id": "NdCcGRqk-pzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Each image is in the range [0,255]. We will normalize to [-1,1].\n",
        "# This helps the generator learn better.\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "x_train = (x_train.astype('float32') - 127.5) / 127.5  # Scale to [-1, 1]\n",
        "\n",
        "# Create tf.data.Dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFWCaogByN9P",
        "outputId": "32dd04e9-d3cd-49d3-d02a-d565afa6ecfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Generator Model\n",
        "##    - Input: random noise vector of dimension NOISE_DIM\n",
        "##    - Output: 32x32x3 image"
      ],
      "metadata": {
        "id": "khFIYQ5u-wxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential(name=\"Generator\")\n",
        "\n",
        "    # 1) Start with a dense layer to project and reshape.\n",
        "    model.add(layers.Dense(4*4*512, use_bias=False, input_shape=(NOISE_DIM,)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Reshape((4, 4, 512)))\n",
        "\n",
        "    # 2) Upsampling to 8x8\n",
        "    model.add(layers.Conv2DTranspose(256, (4, 4), strides=(2, 2), padding='same', use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    # 3) Upsampling to 16x16\n",
        "    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    # 4) Upsampling to 32x32\n",
        "    model.add(layers.Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same', use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    # 5) Final layer: produce 32x32x3, with tanh activation for output in [-1,1].\n",
        "    model.add(layers.Conv2DTranspose(3, (4, 4), strides=(1, 1), padding='same', use_bias=False,\n",
        "                                     activation='tanh'))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "RZ8DDHAfyRo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Discriminator Model\n",
        "##    - Input: 32x32x3 image (either real or fake)\n",
        "##    - Output: real/fake score (logits)"
      ],
      "metadata": {
        "id": "z7F7EPBcyLfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential(name=\"Discriminator\")\n",
        "\n",
        "    # 1) Downsample: 32x32x3 -> 16x16x64\n",
        "    model.add(layers.Conv2D(64, (4, 4), strides=(2, 2), padding='same',\n",
        "                            input_shape=(32, 32, 3)))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    # 2) Downsample: 16x16x64 -> 8x8x128\n",
        "    model.add(layers.Conv2D(128, (4, 4), strides=(2, 2), padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    # 3) Downsample: 8x8x128 -> 4x4x256\n",
        "    model.add(layers.Conv2D(256, (4, 4), strides=(2, 2), padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    # Flatten and final dense for classification\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1))  # Outputs a single logit\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "V-zR60lTyYQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Loss Functions & Optimizers"
      ],
      "metadata": {
        "id": "86mbr2cY-8cC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_disc_loss = real_loss + fake_loss\n",
        "    return total_disc_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    # We want fake_output to be classified as real, so labels=1.\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "generator = make_generator_model()\n",
        "discriminator = make_discriminator_model()\n",
        "\n",
        "# Adam optimizer\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5)"
      ],
      "metadata": {
        "id": "xxTE5a_QybRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LapRRBKu_Juz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Image Saving Utility"
      ],
      "metadata": {
        "id": "v6c94LSq_HXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = tf.random.normal([NUM_EXAMPLES_TO_GENERATE, NOISE_DIM])\n",
        "\n",
        "# Directory to save generated images\n",
        "if not os.path.exists('generated_images'):\n",
        "    os.makedirs('generated_images')\n",
        "\n",
        "def generate_and_save_images(model, epoch, test_input):\n",
        "    \"\"\"Generate and save images to disk for visualization.\"\"\"\n",
        "    predictions = model(test_input, training=False)\n",
        "\n",
        "    # Rescale from [-1, 1] -> [0, 1] for display\n",
        "    predictions = (predictions + 1) / 2.0\n",
        "\n",
        "    fig = plt.figure(figsize=(4,4))\n",
        "    for i in range(predictions.shape[0]):\n",
        "        plt.subplot(4, 4, i+1)\n",
        "        plt.imshow(predictions[i])\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'generated_images/image_at_epoch_{epoch:03d}.png')\n",
        "    plt.close(fig)"
      ],
      "metadata": {
        "id": "eVh3VHYzycG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Single Training Step"
      ],
      "metadata": {
        "id": "wPYmq2x7_Ofx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(real_images):\n",
        "    noise = tf.random.normal([BATCH_SIZE, NOISE_DIM])\n",
        "\n",
        "    # Record operations for gradient computation\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        generated_images = generator(noise, training=True)\n",
        "\n",
        "        real_output = discriminator(real_images, training=True)\n",
        "        fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "        gen_loss  = generator_loss(fake_output)\n",
        "        disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    # Compute and apply gradients for generator\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "\n",
        "    # Compute and apply gradients for discriminator\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator,\n",
        "                                                discriminator.trainable_variables))\n",
        "\n",
        "    return gen_loss, disc_loss\n"
      ],
      "metadata": {
        "id": "4B9X_KBhycJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Full Training Loop"
      ],
      "metadata": {
        "id": "TBJ4EoIr_TZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataset, epochs):\n",
        "    for epoch in range(1, epochs+1):\n",
        "        gen_loss_list = []\n",
        "        disc_loss_list = []\n",
        "\n",
        "        for batch_images in dataset:\n",
        "            g_loss, d_loss = train_step(batch_images)\n",
        "            gen_loss_list.append(g_loss)\n",
        "            disc_loss_list.append(d_loss)\n",
        "\n",
        "        # Simple tracking of epoch losses\n",
        "        avg_g_loss = np.mean(gen_loss_list)\n",
        "        avg_d_loss = np.mean(disc_loss_list)\n",
        "        print(f\"Epoch {epoch}/{epochs} | G Loss: {avg_g_loss:.4f} | D Loss: {avg_d_loss:.4f}\")\n",
        "\n",
        "        # Save images every 'SAVE_EVERY' epochs\n",
        "        if epoch % SAVE_EVERY == 0:\n",
        "            generate_and_save_images(generator, epoch, seed)\n",
        "\n",
        "    # Final save at the end of training\n",
        "    generate_and_save_images(generator, epochs, seed)\n"
      ],
      "metadata": {
        "id": "u5RqDwoyycNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Kick off training"
      ],
      "metadata": {
        "id": "Z_KSfq-B_Ya4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train(train_dataset, EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJSBmS1Ty-Uy",
        "outputId": "332d7c66-3629-46be-a9be-18252a90b1ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300 | G Loss: 0.9925 | D Loss: 1.1687\n",
            "Epoch 2/300 | G Loss: 0.7111 | D Loss: 1.3840\n",
            "Epoch 3/300 | G Loss: 0.7231 | D Loss: 1.3679\n",
            "Epoch 4/300 | G Loss: 0.7129 | D Loss: 1.3737\n",
            "Epoch 5/300 | G Loss: 0.7033 | D Loss: 1.3846\n",
            "Epoch 6/300 | G Loss: 0.7158 | D Loss: 1.3746\n",
            "Epoch 7/300 | G Loss: 0.7069 | D Loss: 1.3804\n",
            "Epoch 8/300 | G Loss: 0.7021 | D Loss: 1.3789\n",
            "Epoch 9/300 | G Loss: 0.7117 | D Loss: 1.3783\n",
            "Epoch 10/300 | G Loss: 0.6922 | D Loss: 1.3841\n",
            "Epoch 11/300 | G Loss: 0.7019 | D Loss: 1.3843\n",
            "Epoch 12/300 | G Loss: 0.7035 | D Loss: 1.3789\n",
            "Epoch 13/300 | G Loss: 0.7029 | D Loss: 1.3724\n",
            "Epoch 14/300 | G Loss: 0.7029 | D Loss: 1.3792\n",
            "Epoch 15/300 | G Loss: 0.7006 | D Loss: 1.3716\n",
            "Epoch 16/300 | G Loss: 0.7116 | D Loss: 1.3701\n",
            "Epoch 17/300 | G Loss: 0.6976 | D Loss: 1.3771\n",
            "Epoch 18/300 | G Loss: 0.7295 | D Loss: 1.3519\n",
            "Epoch 19/300 | G Loss: 0.7065 | D Loss: 1.3807\n",
            "Epoch 20/300 | G Loss: 0.7132 | D Loss: 1.3768\n",
            "Epoch 21/300 | G Loss: 0.7147 | D Loss: 1.3751\n",
            "Epoch 22/300 | G Loss: 0.7266 | D Loss: 1.3719\n",
            "Epoch 23/300 | G Loss: 0.7137 | D Loss: 1.3708\n",
            "Epoch 24/300 | G Loss: 0.7226 | D Loss: 1.3720\n",
            "Epoch 25/300 | G Loss: 0.7163 | D Loss: 1.3618\n",
            "Epoch 26/300 | G Loss: 0.7232 | D Loss: 1.3630\n",
            "Epoch 27/300 | G Loss: 0.7332 | D Loss: 1.3553\n",
            "Epoch 28/300 | G Loss: 0.7196 | D Loss: 1.3611\n",
            "Epoch 29/300 | G Loss: 0.7453 | D Loss: 1.3496\n",
            "Epoch 30/300 | G Loss: 0.7641 | D Loss: 1.3215\n",
            "Epoch 31/300 | G Loss: 0.7302 | D Loss: 1.3460\n",
            "Epoch 32/300 | G Loss: 0.7335 | D Loss: 1.3486\n",
            "Epoch 33/300 | G Loss: 0.7573 | D Loss: 1.3362\n",
            "Epoch 34/300 | G Loss: 0.7641 | D Loss: 1.3225\n",
            "Epoch 35/300 | G Loss: 0.7567 | D Loss: 1.3250\n",
            "Epoch 36/300 | G Loss: 0.7859 | D Loss: 1.3091\n",
            "Epoch 37/300 | G Loss: 0.7770 | D Loss: 1.3056\n",
            "Epoch 38/300 | G Loss: 0.8268 | D Loss: 1.2498\n",
            "Epoch 39/300 | G Loss: 0.8172 | D Loss: 1.2930\n",
            "Epoch 40/300 | G Loss: 0.8294 | D Loss: 1.2707\n",
            "Epoch 41/300 | G Loss: 0.8018 | D Loss: 1.2964\n",
            "Epoch 42/300 | G Loss: 0.8560 | D Loss: 1.2373\n",
            "Epoch 43/300 | G Loss: 0.8992 | D Loss: 1.2127\n",
            "Epoch 44/300 | G Loss: 0.8688 | D Loss: 1.2424\n",
            "Epoch 45/300 | G Loss: 0.8606 | D Loss: 1.2438\n",
            "Epoch 46/300 | G Loss: 0.9234 | D Loss: 1.2185\n",
            "Epoch 47/300 | G Loss: 0.8939 | D Loss: 1.2264\n",
            "Epoch 48/300 | G Loss: 0.8788 | D Loss: 1.2327\n",
            "Epoch 49/300 | G Loss: 0.8923 | D Loss: 1.2327\n",
            "Epoch 50/300 | G Loss: 0.9211 | D Loss: 1.2085\n",
            "Epoch 51/300 | G Loss: 0.9219 | D Loss: 1.1933\n",
            "Epoch 52/300 | G Loss: 0.9180 | D Loss: 1.1957\n",
            "Epoch 53/300 | G Loss: 0.9289 | D Loss: 1.1834\n",
            "Epoch 54/300 | G Loss: 0.9398 | D Loss: 1.1736\n",
            "Epoch 55/300 | G Loss: 0.9868 | D Loss: 1.1593\n",
            "Epoch 56/300 | G Loss: 0.9817 | D Loss: 1.1579\n",
            "Epoch 57/300 | G Loss: 0.9824 | D Loss: 1.1579\n",
            "Epoch 58/300 | G Loss: 0.9625 | D Loss: 1.1588\n",
            "Epoch 59/300 | G Loss: 1.0348 | D Loss: 1.1211\n",
            "Epoch 60/300 | G Loss: 1.0653 | D Loss: 1.1177\n",
            "Epoch 61/300 | G Loss: 1.0537 | D Loss: 1.1111\n",
            "Epoch 62/300 | G Loss: 1.0038 | D Loss: 1.1203\n",
            "Epoch 63/300 | G Loss: 1.0638 | D Loss: 1.1098\n",
            "Epoch 64/300 | G Loss: 1.0454 | D Loss: 1.0987\n",
            "Epoch 65/300 | G Loss: 1.0517 | D Loss: 1.1466\n",
            "Epoch 66/300 | G Loss: 1.0097 | D Loss: 1.1572\n",
            "Epoch 67/300 | G Loss: 1.0163 | D Loss: 1.1489\n",
            "Epoch 68/300 | G Loss: 1.0769 | D Loss: 1.1410\n",
            "Epoch 69/300 | G Loss: 0.9868 | D Loss: 1.1545\n",
            "Epoch 70/300 | G Loss: 0.9804 | D Loss: 1.1593\n",
            "Epoch 71/300 | G Loss: 0.9744 | D Loss: 1.1825\n",
            "Epoch 72/300 | G Loss: 1.0107 | D Loss: 1.1623\n",
            "Epoch 73/300 | G Loss: 0.9615 | D Loss: 1.1805\n",
            "Epoch 74/300 | G Loss: 0.9701 | D Loss: 1.1771\n",
            "Epoch 75/300 | G Loss: 0.9774 | D Loss: 1.1951\n",
            "Epoch 76/300 | G Loss: 1.0000 | D Loss: 1.1530\n",
            "Epoch 77/300 | G Loss: 0.9832 | D Loss: 1.1478\n",
            "Epoch 78/300 | G Loss: 1.0012 | D Loss: 1.1561\n",
            "Epoch 79/300 | G Loss: 1.0030 | D Loss: 1.1375\n",
            "Epoch 80/300 | G Loss: 0.9761 | D Loss: 1.1822\n",
            "Epoch 81/300 | G Loss: 0.9729 | D Loss: 1.1820\n",
            "Epoch 82/300 | G Loss: 0.9585 | D Loss: 1.2036\n",
            "Epoch 83/300 | G Loss: 0.9856 | D Loss: 1.1794\n",
            "Epoch 84/300 | G Loss: 1.0598 | D Loss: 1.1345\n",
            "Epoch 85/300 | G Loss: 0.9944 | D Loss: 1.1456\n",
            "Epoch 86/300 | G Loss: 0.9585 | D Loss: 1.1840\n",
            "Epoch 87/300 | G Loss: 0.9355 | D Loss: 1.2168\n",
            "Epoch 88/300 | G Loss: 0.9641 | D Loss: 1.1693\n",
            "Epoch 89/300 | G Loss: 0.9504 | D Loss: 1.1932\n",
            "Epoch 90/300 | G Loss: 0.9457 | D Loss: 1.1981\n",
            "Epoch 91/300 | G Loss: 0.9501 | D Loss: 1.1790\n",
            "Epoch 92/300 | G Loss: 0.9577 | D Loss: 1.1792\n",
            "Epoch 93/300 | G Loss: 0.9357 | D Loss: 1.1993\n",
            "Epoch 94/300 | G Loss: 0.9525 | D Loss: 1.2055\n",
            "Epoch 95/300 | G Loss: 0.9247 | D Loss: 1.2072\n",
            "Epoch 96/300 | G Loss: 0.9596 | D Loss: 1.1948\n",
            "Epoch 97/300 | G Loss: 0.9323 | D Loss: 1.2011\n",
            "Epoch 98/300 | G Loss: 0.9627 | D Loss: 1.1905\n",
            "Epoch 99/300 | G Loss: 0.9200 | D Loss: 1.2112\n",
            "Epoch 100/300 | G Loss: 0.9284 | D Loss: 1.2108\n",
            "Epoch 101/300 | G Loss: 0.9029 | D Loss: 1.2337\n",
            "Epoch 102/300 | G Loss: 0.8882 | D Loss: 1.2382\n",
            "Epoch 103/300 | G Loss: 0.8759 | D Loss: 1.2514\n",
            "Epoch 104/300 | G Loss: 0.9129 | D Loss: 1.2384\n",
            "Epoch 105/300 | G Loss: 0.9074 | D Loss: 1.2234\n",
            "Epoch 106/300 | G Loss: 0.8933 | D Loss: 1.2366\n",
            "Epoch 107/300 | G Loss: 0.8794 | D Loss: 1.2475\n",
            "Epoch 108/300 | G Loss: 0.9020 | D Loss: 1.2318\n",
            "Epoch 109/300 | G Loss: 0.8893 | D Loss: 1.2364\n",
            "Epoch 110/300 | G Loss: 0.8882 | D Loss: 1.2363\n",
            "Epoch 111/300 | G Loss: 0.8811 | D Loss: 1.2576\n",
            "Epoch 112/300 | G Loss: 0.8798 | D Loss: 1.2351\n",
            "Epoch 113/300 | G Loss: 0.8600 | D Loss: 1.2604\n",
            "Epoch 114/300 | G Loss: 0.8658 | D Loss: 1.2626\n",
            "Epoch 115/300 | G Loss: 0.8557 | D Loss: 1.2663\n",
            "Epoch 116/300 | G Loss: 0.8464 | D Loss: 1.2692\n",
            "Epoch 117/300 | G Loss: 0.8608 | D Loss: 1.2532\n",
            "Epoch 118/300 | G Loss: 0.8672 | D Loss: 1.2594\n",
            "Epoch 119/300 | G Loss: 0.8666 | D Loss: 1.2616\n",
            "Epoch 120/300 | G Loss: 0.8641 | D Loss: 1.2657\n",
            "Epoch 121/300 | G Loss: 0.8555 | D Loss: 1.2624\n",
            "Epoch 122/300 | G Loss: 0.8638 | D Loss: 1.2593\n",
            "Epoch 123/300 | G Loss: 0.8514 | D Loss: 1.2707\n",
            "Epoch 124/300 | G Loss: 0.8335 | D Loss: 1.2762\n",
            "Epoch 125/300 | G Loss: 0.8400 | D Loss: 1.2772\n",
            "Epoch 126/300 | G Loss: 0.8508 | D Loss: 1.2762\n",
            "Epoch 127/300 | G Loss: 0.8358 | D Loss: 1.2772\n",
            "Epoch 128/300 | G Loss: 0.8384 | D Loss: 1.2776\n",
            "Epoch 129/300 | G Loss: 0.8457 | D Loss: 1.2704\n",
            "Epoch 130/300 | G Loss: 0.8476 | D Loss: 1.2625\n",
            "Epoch 131/300 | G Loss: 0.8796 | D Loss: 1.2626\n",
            "Epoch 132/300 | G Loss: 0.8524 | D Loss: 1.2641\n",
            "Epoch 133/300 | G Loss: 0.8267 | D Loss: 1.2734\n",
            "Epoch 134/300 | G Loss: 0.8650 | D Loss: 1.2814\n",
            "Epoch 135/300 | G Loss: 0.8166 | D Loss: 1.2870\n",
            "Epoch 136/300 | G Loss: 0.8194 | D Loss: 1.2916\n",
            "Epoch 137/300 | G Loss: 0.8371 | D Loss: 1.2766\n",
            "Epoch 138/300 | G Loss: 0.8206 | D Loss: 1.2850\n",
            "Epoch 139/300 | G Loss: 0.8274 | D Loss: 1.2836\n",
            "Epoch 140/300 | G Loss: 0.8353 | D Loss: 1.2796\n",
            "Epoch 141/300 | G Loss: 0.8170 | D Loss: 1.2937\n",
            "Epoch 142/300 | G Loss: 0.8152 | D Loss: 1.2962\n",
            "Epoch 143/300 | G Loss: 0.8145 | D Loss: 1.2991\n",
            "Epoch 144/300 | G Loss: 0.8121 | D Loss: 1.2916\n",
            "Epoch 145/300 | G Loss: 0.8249 | D Loss: 1.2799\n",
            "Epoch 146/300 | G Loss: 0.9387 | D Loss: 1.2689\n",
            "Epoch 147/300 | G Loss: 0.8277 | D Loss: 1.2803\n",
            "Epoch 148/300 | G Loss: 0.8060 | D Loss: 1.2990\n",
            "Epoch 149/300 | G Loss: 0.7964 | D Loss: 1.3023\n",
            "Epoch 150/300 | G Loss: 0.8065 | D Loss: 1.3012\n",
            "Epoch 151/300 | G Loss: 0.8153 | D Loss: 1.2887\n",
            "Epoch 152/300 | G Loss: 0.8171 | D Loss: 1.3036\n",
            "Epoch 153/300 | G Loss: 0.8095 | D Loss: 1.2992\n",
            "Epoch 154/300 | G Loss: 0.8056 | D Loss: 1.3013\n",
            "Epoch 155/300 | G Loss: 0.8043 | D Loss: 1.2999\n",
            "Epoch 156/300 | G Loss: 0.7992 | D Loss: 1.3053\n",
            "Epoch 157/300 | G Loss: 0.8150 | D Loss: 1.2951\n",
            "Epoch 158/300 | G Loss: 0.8490 | D Loss: 1.2850\n",
            "Epoch 159/300 | G Loss: 0.8010 | D Loss: 1.3074\n",
            "Epoch 160/300 | G Loss: 0.8213 | D Loss: 1.3008\n",
            "Epoch 161/300 | G Loss: 0.7909 | D Loss: 1.3087\n",
            "Epoch 162/300 | G Loss: 0.8113 | D Loss: 1.3162\n",
            "Epoch 163/300 | G Loss: 0.8063 | D Loss: 1.2999\n",
            "Epoch 164/300 | G Loss: 0.7966 | D Loss: 1.3080\n",
            "Epoch 165/300 | G Loss: 0.7912 | D Loss: 1.3130\n",
            "Epoch 166/300 | G Loss: 0.7896 | D Loss: 1.3083\n",
            "Epoch 167/300 | G Loss: 0.8088 | D Loss: 1.3063\n",
            "Epoch 168/300 | G Loss: 0.8341 | D Loss: 1.2921\n",
            "Epoch 169/300 | G Loss: 0.8110 | D Loss: 1.3163\n",
            "Epoch 170/300 | G Loss: 0.8403 | D Loss: 1.2871\n",
            "Epoch 171/300 | G Loss: 0.8028 | D Loss: 1.3026\n",
            "Epoch 172/300 | G Loss: 0.7861 | D Loss: 1.3104\n",
            "Epoch 173/300 | G Loss: 0.8022 | D Loss: 1.2988\n",
            "Epoch 174/300 | G Loss: 0.8025 | D Loss: 1.3019\n",
            "Epoch 175/300 | G Loss: 0.8179 | D Loss: 1.3101\n",
            "Epoch 176/300 | G Loss: 0.8182 | D Loss: 1.2901\n",
            "Epoch 177/300 | G Loss: 0.8178 | D Loss: 1.2999\n",
            "Epoch 178/300 | G Loss: 0.7889 | D Loss: 1.3090\n",
            "Epoch 179/300 | G Loss: 0.7834 | D Loss: 1.3128\n",
            "Epoch 180/300 | G Loss: 0.8091 | D Loss: 1.3155\n",
            "Epoch 181/300 | G Loss: 0.8063 | D Loss: 1.3103\n",
            "Epoch 182/300 | G Loss: 0.8018 | D Loss: 1.3027\n",
            "Epoch 183/300 | G Loss: 0.7945 | D Loss: 1.3064\n",
            "Epoch 184/300 | G Loss: 0.8001 | D Loss: 1.3092\n",
            "Epoch 185/300 | G Loss: 0.7850 | D Loss: 1.3089\n",
            "Epoch 186/300 | G Loss: 0.8217 | D Loss: 1.3091\n",
            "Epoch 187/300 | G Loss: 0.8110 | D Loss: 1.2999\n",
            "Epoch 188/300 | G Loss: 0.8026 | D Loss: 1.3063\n",
            "Epoch 189/300 | G Loss: 0.8118 | D Loss: 1.2962\n",
            "Epoch 190/300 | G Loss: 0.7892 | D Loss: 1.3087\n",
            "Epoch 191/300 | G Loss: 0.7851 | D Loss: 1.3146\n",
            "Epoch 192/300 | G Loss: 0.8017 | D Loss: 1.3010\n",
            "Epoch 193/300 | G Loss: 0.8178 | D Loss: 1.2975\n",
            "Epoch 194/300 | G Loss: 0.8118 | D Loss: 1.2995\n",
            "Epoch 195/300 | G Loss: 0.7861 | D Loss: 1.3122\n",
            "Epoch 196/300 | G Loss: 0.7928 | D Loss: 1.3031\n",
            "Epoch 197/300 | G Loss: 0.8382 | D Loss: 1.2944\n",
            "Epoch 198/300 | G Loss: 0.8082 | D Loss: 1.3087\n",
            "Epoch 199/300 | G Loss: 0.7907 | D Loss: 1.3094\n",
            "Epoch 200/300 | G Loss: 0.7949 | D Loss: 1.3094\n",
            "Epoch 201/300 | G Loss: 0.7985 | D Loss: 1.3052\n",
            "Epoch 202/300 | G Loss: 0.8283 | D Loss: 1.3616\n",
            "Epoch 203/300 | G Loss: 0.8179 | D Loss: 1.3027\n",
            "Epoch 204/300 | G Loss: 0.7856 | D Loss: 1.3067\n",
            "Epoch 205/300 | G Loss: 0.7848 | D Loss: 1.3059\n",
            "Epoch 206/300 | G Loss: 0.7823 | D Loss: 1.3180\n",
            "Epoch 207/300 | G Loss: 0.7870 | D Loss: 1.3116\n",
            "Epoch 208/300 | G Loss: 0.7824 | D Loss: 1.3111\n",
            "Epoch 209/300 | G Loss: 0.8229 | D Loss: 1.2971\n",
            "Epoch 210/300 | G Loss: 0.8182 | D Loss: 1.3066\n",
            "Epoch 211/300 | G Loss: 0.7904 | D Loss: 1.3084\n",
            "Epoch 212/300 | G Loss: 0.7825 | D Loss: 1.3169\n",
            "Epoch 213/300 | G Loss: 0.7752 | D Loss: 1.3209\n",
            "Epoch 214/300 | G Loss: 0.7947 | D Loss: 1.3145\n",
            "Epoch 215/300 | G Loss: 0.7851 | D Loss: 1.3185\n",
            "Epoch 216/300 | G Loss: 0.7870 | D Loss: 1.3154\n",
            "Epoch 217/300 | G Loss: 0.7831 | D Loss: 1.3196\n",
            "Epoch 218/300 | G Loss: 0.7974 | D Loss: 1.3133\n",
            "Epoch 219/300 | G Loss: 0.7779 | D Loss: 1.3248\n",
            "Epoch 220/300 | G Loss: 0.7751 | D Loss: 1.3246\n",
            "Epoch 221/300 | G Loss: 0.7785 | D Loss: 1.3253\n",
            "Epoch 222/300 | G Loss: 0.7817 | D Loss: 1.3203\n",
            "Epoch 223/300 | G Loss: 0.7858 | D Loss: 1.3218\n",
            "Epoch 224/300 | G Loss: 0.8053 | D Loss: 1.3140\n",
            "Epoch 225/300 | G Loss: 0.7779 | D Loss: 1.3228\n",
            "Epoch 226/300 | G Loss: 0.7696 | D Loss: 1.3281\n",
            "Epoch 227/300 | G Loss: 0.7678 | D Loss: 1.3268\n",
            "Epoch 228/300 | G Loss: 0.7800 | D Loss: 1.3218\n",
            "Epoch 229/300 | G Loss: 0.7868 | D Loss: 1.3199\n",
            "Epoch 230/300 | G Loss: 0.7759 | D Loss: 1.3230\n",
            "Epoch 231/300 | G Loss: 0.7819 | D Loss: 1.3262\n",
            "Epoch 232/300 | G Loss: 0.7941 | D Loss: 1.3146\n",
            "Epoch 233/300 | G Loss: 0.7826 | D Loss: 1.3228\n",
            "Epoch 234/300 | G Loss: 0.7763 | D Loss: 1.3203\n",
            "Epoch 235/300 | G Loss: 0.7820 | D Loss: 1.3137\n",
            "Epoch 236/300 | G Loss: 0.7719 | D Loss: 1.3219\n",
            "Epoch 237/300 | G Loss: 0.7885 | D Loss: 1.3190\n",
            "Epoch 238/300 | G Loss: 0.7804 | D Loss: 1.3211\n",
            "Epoch 239/300 | G Loss: 0.7748 | D Loss: 1.3205\n",
            "Epoch 240/300 | G Loss: 0.7747 | D Loss: 1.3237\n",
            "Epoch 241/300 | G Loss: 0.7772 | D Loss: 1.3189\n",
            "Epoch 242/300 | G Loss: 0.7971 | D Loss: 1.3180\n",
            "Epoch 243/300 | G Loss: 0.7748 | D Loss: 1.3227\n",
            "Epoch 244/300 | G Loss: 0.7848 | D Loss: 1.3181\n",
            "Epoch 245/300 | G Loss: 0.7827 | D Loss: 1.3167\n",
            "Epoch 246/300 | G Loss: 0.7830 | D Loss: 1.3155\n",
            "Epoch 247/300 | G Loss: 0.7861 | D Loss: 1.3149\n",
            "Epoch 248/300 | G Loss: 0.7904 | D Loss: 1.3118\n",
            "Epoch 249/300 | G Loss: 0.7745 | D Loss: 1.3218\n",
            "Epoch 250/300 | G Loss: 0.7825 | D Loss: 1.3158\n",
            "Epoch 251/300 | G Loss: 0.7844 | D Loss: 1.3172\n",
            "Epoch 252/300 | G Loss: 0.7839 | D Loss: 1.3149\n",
            "Epoch 253/300 | G Loss: 0.7840 | D Loss: 1.3165\n",
            "Epoch 254/300 | G Loss: 0.7863 | D Loss: 1.3200\n",
            "Epoch 255/300 | G Loss: 0.7859 | D Loss: 1.3164\n",
            "Epoch 256/300 | G Loss: 0.7816 | D Loss: 1.3168\n",
            "Epoch 257/300 | G Loss: 0.7807 | D Loss: 1.3199\n",
            "Epoch 258/300 | G Loss: 0.7787 | D Loss: 1.3167\n",
            "Epoch 259/300 | G Loss: 0.7955 | D Loss: 1.3154\n",
            "Epoch 260/300 | G Loss: 0.7997 | D Loss: 1.3105\n",
            "Epoch 261/300 | G Loss: 0.7827 | D Loss: 1.3172\n",
            "Epoch 262/300 | G Loss: 0.7793 | D Loss: 1.3175\n",
            "Epoch 263/300 | G Loss: 0.7745 | D Loss: 1.3201\n",
            "Epoch 264/300 | G Loss: 0.7830 | D Loss: 1.3156\n",
            "Epoch 265/300 | G Loss: 0.7806 | D Loss: 1.3229\n",
            "Epoch 266/300 | G Loss: 0.7857 | D Loss: 1.3120\n",
            "Epoch 267/300 | G Loss: 0.7903 | D Loss: 1.3116\n",
            "Epoch 268/300 | G Loss: 0.7891 | D Loss: 1.3125\n",
            "Epoch 269/300 | G Loss: 0.7954 | D Loss: 1.3065\n",
            "Epoch 270/300 | G Loss: 0.7917 | D Loss: 1.3082\n",
            "Epoch 271/300 | G Loss: 0.7906 | D Loss: 1.3087\n",
            "Epoch 272/300 | G Loss: 0.7888 | D Loss: 1.3115\n",
            "Epoch 273/300 | G Loss: 0.7872 | D Loss: 1.3140\n",
            "Epoch 274/300 | G Loss: 0.7826 | D Loss: 1.3150\n",
            "Epoch 275/300 | G Loss: 0.7945 | D Loss: 1.3112\n",
            "Epoch 276/300 | G Loss: 0.7863 | D Loss: 1.3106\n",
            "Epoch 277/300 | G Loss: 0.7954 | D Loss: 1.3070\n",
            "Epoch 278/300 | G Loss: 0.7939 | D Loss: 1.3103\n",
            "Epoch 279/300 | G Loss: 0.7878 | D Loss: 1.3080\n",
            "Epoch 280/300 | G Loss: 0.7927 | D Loss: 1.3208\n",
            "Epoch 281/300 | G Loss: 0.7932 | D Loss: 1.3077\n",
            "Epoch 282/300 | G Loss: 0.7930 | D Loss: 1.3064\n",
            "Epoch 283/300 | G Loss: 0.7935 | D Loss: 1.3064\n",
            "Epoch 284/300 | G Loss: 0.7896 | D Loss: 1.3108\n",
            "Epoch 285/300 | G Loss: 0.7998 | D Loss: 1.3024\n",
            "Epoch 286/300 | G Loss: 0.7995 | D Loss: 1.3024\n",
            "Epoch 287/300 | G Loss: 0.7927 | D Loss: 1.3065\n",
            "Epoch 288/300 | G Loss: 0.7941 | D Loss: 1.3057\n",
            "Epoch 289/300 | G Loss: 0.7969 | D Loss: 1.3062\n",
            "Epoch 290/300 | G Loss: 0.7923 | D Loss: 1.3121\n",
            "Epoch 291/300 | G Loss: 0.8006 | D Loss: 1.3021\n",
            "Epoch 292/300 | G Loss: 0.8159 | D Loss: 1.3081\n",
            "Epoch 293/300 | G Loss: 0.7907 | D Loss: 1.3053\n",
            "Epoch 294/300 | G Loss: 0.7905 | D Loss: 1.3050\n",
            "Epoch 295/300 | G Loss: 0.7982 | D Loss: 1.3051\n",
            "Epoch 296/300 | G Loss: 0.8075 | D Loss: 1.3118\n",
            "Epoch 297/300 | G Loss: 0.8090 | D Loss: 1.3016\n",
            "Epoch 298/300 | G Loss: 0.7910 | D Loss: 1.3052\n",
            "Epoch 299/300 | G Loss: 0.8033 | D Loss: 1.3006\n",
            "Epoch 300/300 | G Loss: 0.7963 | D Loss: 1.3039\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qzLJWhcJbngU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}